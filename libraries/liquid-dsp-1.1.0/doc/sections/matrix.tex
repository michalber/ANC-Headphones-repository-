% 
% MODULE : matrix
%

\newpage
\section{matrix}
\label{module:matrix}
Matrices are used for solving linear systems of equations and are used
extensively in polynomial fitting, adaptive equalization, and filter design.
In {\it liquid}, matrices are represented as just arrays of a single dimension,
and do not rely on special objects for their manipulation.
This is to help portability of the code and ease of integration into other
libraries.
Here is a simple example of the matrix interface:
%
\input{listings/matrix.example.c.tex}
%
Notice that all routines for the type {\it float} are prefaced with
{\tt matrixf}.
This follows the naming convention of the standard C library routines which
append an {\tt f} to the end of methods operating on floating-point precision
types.
Similar matrix interfaces exist in \liquid\ for
{\it double} ({\tt matrix}),
{\it double complex} ({\tt matrixc}), and
{\it float complex} ({\tt matrixcf}).

% 
% basic math operations 
%
\subsection{Basic math operations}
\label{module:matrix:math}
This section describes the basic matrix math operations, including addition,
subtraction, point-wise multiplication and division, transposition, and
initializing the identity matrix.

\subsubsection{{\tt matrix\_access} (access element)}
\label{module:matrix:access}
Because matrices in \liquid\ are really just one-dimensional arrays, indexing
matrix values for storage or retrieval is as straightforward as indexing the
array itself.
\liquid\ also provides a simple macro for ensuring the proper value is
returned.
{\tt matrix\_access(X,R,C,r,c)} will access the element of a $R \times C$
matrix $\vec{X}$ at row $r$ and column $c$.
This method is really just a pre-processor macro which performs a literal
string replacement
\begin{Verbatim}[fontsize=\small]
  #define matrix_access(X,R,C,r,c) ((X)[(r)*(C)+(c)])
\end{Verbatim}
and can be used for both setting and retrieving values of a matrix.
For example,
\begin{Verbatim}[fontsize=\small]
    X = 
      0.406911   0.118444   0.923281   0.827254   0.463265
      0.038897   0.132381   0.061137   0.880045   0.570341
      0.151206   0.439508   0.695207   0.215935   0.999683
      0.808384   0.601597   0.149171   0.975722   0.205819

    float v = matrix_access(X,4,5,0,1);
    v =
      0.118444

    matrix_access(X,4,5,2,3) = 0;
    X =
      0.406911   0.118444   0.923281   0.827254   0.463265
      0.038897   0.132381   0.061137   0.880045   0.570341
      0.151206   0.439508   0.695207   0.0        0.999683
      0.808384   0.601597   0.149171   0.975722   0.205819
\end{Verbatim}
Because this method is really just a macro, there is no error-checking to
ensure that one is accessing the matrix within its memory bounds.
Therefore, special care must be taken when programming.
Furthermore, {\tt matrix\_access()} can be used for all matrix types
({\tt matrixf}, {\tt matrixcf}, etc.).

\subsubsection{{\tt matrixf\_add}, {\tt matrixf\_sub}, {\tt matrixf\_pmul},
and {\tt matrixf\_pdiv} (scalar math operations)}
\label{module:matrix:mathop}
The {\tt matrixf\_add(*x,*y,*z,m,n)},
    {\tt matrixf\_sub(*x,*y,*z,m,n)},
    {\tt matrixf\_pmul(*x,*y,*z,m,n)}, and
    {\tt matrixf\_pdiv(*x,*y,*z,m,n)} methods perform
point-wise (scalar) addition, subtraction, multiplication, and division
of the elements of two $n \times m$ matrices, $\vec{X}$ and $\vec{Y}$.
That is, $\vec{Z}_{i,k} = \vec{X}_{i,k} + \vec{Y}_{i,k}$ for all $i$, $k$.
The same holds true for subtraction, multiplication, and division.
It is very important to understand the difference between the methods
{\tt matrixf\_pmul()} and {\tt matrixf\_mul()}, as well as
{\tt matrixf\_pdiv()} and {\tt matrixf\_div()}.
In each case the latter performs a vastly different operation from {\tt
matrixf\_mul()} and {\tt matrixf\_div()}
(see Sections~\ref{module:matrix:mul} and \ref{module:matrix:div},
respectively).
%
\begin{Verbatim}[fontsize=\small]
    X =                         Y =
      0.59027   0.83429           0.764108   0.741641
      0.67779   0.19793           0.660932   0.041723
      0.95075   0.33980           0.972282   0.347090

    matrixf_pmul(X,Y,Z,2,3);
    Z =
      0.4510300   0.6187437
      0.4479731   0.0082582
      0.9243971   0.1179412
\end{Verbatim}
%

\subsubsection{{\tt matrixf\_trans()}, {\tt matrixf\_hermitian()} (transpose matrix)}
\label{module:matrix:trans}
The {\tt matrixf\_trans(X,m,n,XT)} method
performs the conjugate matrix transpose operation on an
$m \times n$ matrix $\vec{X}$.
That is, the matrix is flipped on its main diagonal and the conjugate of
each element is taken.
Formally, $\vec{A}^T_{i,j} = \vec{A}_{j,i}^*$.
Here's a simple example:
\[
    \left[
    \begin{array}{ccc}
    0 & 1 & 2 \\
    3 & 4 & 5 \\
    \end{array}
    \right]^T
    =
    \left[
    \begin{array}{cc}
    0 & 3 \\
    1 & 4 \\
    2 & 5 \\
    \end{array}
    \right]
\]
%
Similarly, the {\tt matrixf\_hermitian(X,m,n,XH)} computes the Hermitian
transpose which is identical to the regular transpose but without the
conjugation operation, viz
$\vec{A}^H_{i,j} = \vec{A}_{j,i}$.

\subsubsection{{\tt matrixf\_eye()} (identity matrix)}
\label{module:matrix:eye}
The {\tt matrixf\_eye(*x,n)} method generates the $n \times n$ identity
matrix $\vec{I}_n$:
%
\begin{equation}
\label{eqn:matrix:eye}
    \vec{I}_n = 
    \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        %  &   &        &   \\
        \\
        0 & 0 & \cdots & 1 \\
    \end{bmatrix}
\end{equation}

% 
% elementary math operations 
%
\subsection{Elementary math operations}
\label{module:matrix:elementary}
This section describes elementary math operations for linear systems of
equations.

\subsubsection{{\tt matrixf\_swaprows()} (swap rows)}
\label{module:matrix:swaprows}
Matrix row-swapping is often necessary to express a matrix in its
row-reduced echelon form.
The {\tt matrixf\_swaprows(*X,m,n,i,j)} method simply swaps rows $i$ and
$j$ of an $m \times n$ matrix $\vec{X}$, viz
%
\begin{Verbatim}[fontsize=\small]
    x = 
      0.84381998 -2.38303995  1.43060994 -1.66603994
      3.99475002  0.88066000  4.69372988  0.44563001
      7.28072023 -2.06608009  0.67074001  9.80657005
      6.07741022 -3.93098998  1.22826004 -0.42142001

    matrixf_swaprows(x,4,4,0,2);
      7.28072023 -2.06608009  0.67074001  9.80657005
      3.99475002  0.88066000  4.69372988  0.44563001
      0.84381998 -2.38303995  1.43060994 -1.66603994
      6.07741022 -3.93098998  1.22826004 -0.42142001
\end{Verbatim}

\subsubsection{{\tt matrixf\_pivot()} (pivoting)}
\label{module:matrix:pivot}
[NOTE: terminology for ``pivot'' is different from literature.]
Given an $n \times m$ matrix $\vec{A}$,
\[
    \vec{A} = 
    \begin{bmatrix}
        A_{0,0}     & A_{0,1}   & \cdots  & A_{0,m-1} \\
        A_{1,0}     & A_{1,1}   & \cdots  & A_{1,m-1} \\
        \\
        A_{n-1,0}   & A_{n-1,1} & \cdots  & A_{n-1,m-1}
    \end{bmatrix}
\]
pivoting $\vec{A}$ around $\vec{A}_{a,b}$ gives
\[
    \vec{B}_{i,j} = \left(
                    \frac{\vec{A}_{i,b}}{\vec{A}_{a,b}}
                    \right)
                    \vec{A}_{a,j} - \vec{A}_{i,j}
                    \forall i \ne a
\]
The pivot element must not be zero.
Row $a$ is left unchanged in $\vec{B}$.
All elements of $\vec{B}$ in column $b$ are zero except for row $a$.
This is accomplished in \liquid\ with the
{\tt matrixf\_pivot(*A,m,n,i,j)} method.
For our example $4 \times 4$ matrix $\vec{x}$, pivoting around
$\vec{x}_{1,2}$ gives:
%
\begin{Verbatim}[fontsize=\small]
    matrixf_pivot(x,4,4,1,2);
      0.37374675  2.65145779  0.00000000  1.80186427
      3.99475002  0.88066000  4.69372988  0.44563001
     -6.70986557  2.19192743  0.00000000 -9.74288940
     -5.03205967  4.16144180  0.00000000  0.53803295
\end{Verbatim}

% matrixf_mul()
\subsubsection{{\tt matrixf\_mul()} (multiplication)}
\label{module:matrix:mul}
Multiplication of two input matrices $\vec{A}$ and $\vec{B}$ is accomplished
with the {\tt matrixf\_mul(*A,ma,na,*B,mb,nb,*C,mc,nc)} method,
and is not to be confused with {\tt matrixf\_pmul()}
in \S\ref{module:matrix:mathop}.
If $\vec{A}$ is $m \times n$ and $\vec{B}$ is $n \times p$, then their product
is computed as
%
\begin{equation}
\label{eqn:matrix:mul}
    \bigl( \vec{A} \vec{B} \bigr)_{i,j}
        = \sum_{r=0}^{n-1} { \vec{A}_{i,r} \vec{B}_{r,j} }
\end{equation}
%
Note that the number of columns of $\vec{A}$ must be equal to the number of
rows of $\vec{B}$, and that the resulting matrix is of size $m \times p$
(the number of rows in $\vec{A}$ and columns in $\vec{B}$).
%
% TODO : come up with better example
\begin{Verbatim}[fontsize=\small]
    A =                 B =
        1   2   3           1   2   3
        4   5   6           4   5   6
                            7   8   9
    matrixf_mul(A,2,3, B,3,3, C,2,3);
    
    C =
        30  36  42
        66  81  96
\end{Verbatim}


% 
% transpose multiplication
%
\subsubsection{Transpose multiplication}
\label{module:matrix:transmul}
\liquid\ also implements transpose-multiplication operations on an
$m \times n$ matrix $\vec{X}$, commonly used in signal processing.
\S\ref{module:matrix:trans} describes the difference between the
$\left(\cdot\right)^T$ and 
$\left(\cdot\right)^H$ operations.
The interface for transpose-multiplications in \liquid\ is tabulated
below for an input $m \times n$ matrix $\vec{X}$.
\\

% ------------ TABLE: MATRIX TRANS-MUL INTERFACE ------------
{\small
    \begin{tabular*}{0.85\textwidth}{l@{\extracolsep{\fill}}ll}
    \toprule
    {\it operation} &
    {\it output dimensions} &
    {\it interface}\\\otoprule
    %
    $\vec{X}  \vec{X}^T$  & $m \times m$  & {\tt matrixcf\_mul\_transpose(x,m,n,xxT)} \\
    $\vec{X}  \vec{X}^H$  & $m \times m$  & {\tt matrixcf\_mul\_hermitian(x,m,n,xxH)} \\
    $\vec{X}^T\vec{X}  $  & $n \times n$  & {\tt matrixcf\_transpose\_mul(x,m,n,xTx)} \\
    $\vec{X}^H\vec{X}  $  & $n \times n$  & {\tt matrixcf\_transpose\_mul(x,m,n,xHx)} \\\bottomrule
    \end{tabular*}
}

% 
% complex math operations 
%
\subsection{Complex math operations}
\label{module:matrix:complex}
More complex math operations are described here, including matrix inversion,
square matrix determinant,
Gauss-Jordan elimination, and lower/upper decomposition routines using both
Crout's and Doolittle's methods.
% singular value decomposition
% eigenvalue decomposition

\subsubsection{{\tt matrixf\_inv} (inverse)}
\label{module:matrix:inv}
Matrix inversion is accomplished with the {\tt matrixf\_inv(*X,m,n)}
method.%
\footnote{While matrix inversion requires a square matrix, \liquid\
          internally checks to ensure $m=n$ on the input size for
          $\vec{X}$.}
Given an $n \times n$ matrix $\vec{A}$, \liquid\ augments with
$\vec{I}_n$:
\[
    \left[\vec{A}|\vec{I}_n\right] = 
    \left[
    \begin{array}{cccc|cccc}
    A_{0,0}     & A_{0,1}   & \cdots  & A_{0,m-1}   & 1 & 0 & \cdots & 0 \\
    A_{1,0}     & A_{1,1}   & \cdots  & A_{1,m-1}   & 0 & 1 & \cdots & 0 \\
                &           &         &             &   &   &        &   \\
    A_{n-1,0}   & A_{n-1,1} & \cdots  & A_{n-1,m-1} & 0 & 0 & \cdots & 1 \\
    \end{array}
    \right]
\]
Next \liquid\ performs elementary operations to convert to its
row-reduced echelon form.
The resulting matrix has the identity matrix on the left and
$\vec{A}^{-1}$ on its right, viz
\[
    \left[\vec{I}_n|\vec{A}^{-1}\right] = 
    \left[
    \begin{array}{cccc|cccc}
1 & 0 & \cdots & 0 & A^{-1}_{0,0}   & A^{-1}_{0,1}   & \cdots  & A^{-1}_{0,m-1}   \\
0 & 1 & \cdots & 0 & A^{-1}_{1,0}   & A^{-1}_{1,1}   & \cdots  & A^{-1}_{1,m-1}   \\
  &   &        &   &                &                &         &                  \\
0 & 0 & \cdots & 1 & A^{-1}_{n-1,0} & A^{-1}_{n-1,1} & \cdots  & A^{-1}_{n-1,m-1} \\
    \end{array}
    \right]
\]
The {\tt matrixf\_inv()} method uses Gauss-Jordan elimination
(see {\tt matrixf\_gjelim()}) for row reduction and back-substitution.
Pivot elements in $\vec{A}$ with the largest magnitude are chosen to
help stability in floating-point arithmetic.
%
\begin{Verbatim}[fontsize=\small]
    matrixf_inv(x,4,4);
     -0.33453920  0.04643385 -0.04868321  0.23879384
     -0.42204019  0.12152659 -0.07431178  0.06774280
      0.35104612  0.15256262  0.04403552 -0.20177667
      0.13544561 -0.01930523  0.11944833 -0.14921521
\end{Verbatim}


\subsubsection{{\tt matrixf\_div()}}
\label{module:matrix:div}
The {\tt matrixf\_div(*X,*Y,*Z,*n)} method simply computes
$\vec{Z} = \vec{Y}^{-1}\vec{X}$
where $\vec{X}$, $\vec{Y}$, and $\vec{Z}$ are all $n \times n$ matrices.


\subsubsection{{\tt matrixf\_linsolve()} (solve linear system of equations)}
\label{module:matrix:linsolve}
The {\tt matrixf\_linsolve(*A,n,*b,*x,opts)} method solves a set of $n$ linear
equations $A\vec{x} = \vec{b}$
where
$A$ is an $n \times n$ matrix, and
$\vec{x}$ and $\vec{b}$ are $n \times 1$ vectors.
The {\tt opts} argument is reserved for future development and can be
ignored by setting to {\tt NULL}.

\subsubsection{{\tt matrixf\_cgsolve()} (solve linear system of equations)}
\label{module:matrix:cgsolve}
The {\tt matrixf\_cgsolve(*A,n,*b,*x,opts)} method solves $Ax = b$
using the conjugate gradient method
where $A$ is an $n \times n$ symmetric positive-definite matrix.
The {\tt opts} argument is reserved for future development and can be
ignored by setting to {\tt NULL}.
%
Listed below is a basic example:
%
\begin{Verbatim}[fontsize=\small]
    A = 
      2.9002075   0.1722705   1.3046706   1.8082311
      0.1722705   1.0730995   0.2497573   0.1470398
      1.3046706   0.2497573   0.8930279   1.1471686
      1.8082311   0.1470398   1.1471686   1.5155975
    b =
     11.7622252
     -1.0541668
      5.7372437
      8.1291904
    matrixf_cgsolve(A,4,4, x_hat, NULL)
    x_hat =
      2.8664699
     -1.8786657
      1.1224079
      1.2764599
\end{Verbatim}
%
For a more complete example, see {\tt examples/cgsolve\_example.c}
located under the main project directory.

\subsubsection{{\tt matrixf\_det()} (determinant)}
\label{module:matrix:det}
The {\tt matrixf\_det(*X,m,n)} method computes the determinant
of an $n \times n$ matrix $\vec{X}$.
In {\it liquid}, the determinant is computed by L/U decomposition of
$\vec{A}$ using Doolittle's method (see {\tt
matrixf\_ludecomp\_doolittle}) and then computing the product of the
diagonal elements of $\vec{U}$, viz
\[
    \det\left(\vec{A}\right) =
    \left|\vec{A}\right| =
    \prod_{k=0}^{n-1}{\vec{U}_{k,k}}
\]
This is equivalent to performing L/U decomposition using Crout's method and
then computing the product of the diagonal elements of $\vec{L}$.
%
\begin{Verbatim}[fontsize=\small]
    matrixf_det(X,4,4) = 585.40289307
\end{Verbatim}

\subsubsection{{\tt matrixf\_ludecomp\_crout()} (LU Decomposition, Crout's Method)}
\label{module:matrix:ludecomp_crout}
Crout's method decomposes a non-singular $n\times n$ matrix $\vec{A}$
into a product of a lower triangular $n \times n$ matrix $\vec{L}$ and
an upper triangular $n \times n$ matrix $\vec{U}$. %NOTE : discuss permutation matrix P
In fact, $\vec{U}$ is a unit upper triangular matrix (its values along
the diagonal are 1).
The {\tt matrixf\_ludecomp\_crout(*A,m,n,*L,*U,*P)} implements Crout's
method.
%
\[
    \vec{L}_{i,k} = \vec{A}_{i,k} -
                    \sum_{t=0}^{k-1}{ \vec{L}_{i,t} \vec{U}_{t,k} }
                    \ \forall k \in \{0,n-1\}, i \in \{k,n-1\}
\]
%
\[
    \vec{U}_{k,j} = \left[
                        \vec{A}_{k,j} -
                        \sum_{t=0}^{k-1}{ \vec{L}_{k,t} \vec{U}_{t,j} }
                    \right] / \vec{L}_{k,k}
                    \ \forall k \in \{0,n-1\}, j \in \{k+1,n-1\}
\]
%
\begin{Verbatim}[fontsize=\small]
    matrixf_ludecomp_crout(X,4,4,L,U,P)
    L =
      0.84381998  0.00000000  0.00000000  0.00000000
      3.99475002 12.16227055  0.00000000  0.00000000
      7.28072023 18.49547005 -8.51144791  0.00000000
      6.07741022 13.23228073 -6.81350422 -6.70173073
    U =
      1.00000000 -2.82410932  1.69539714 -1.97440207
      0.00000000  1.00000000 -0.17093502  0.68514121
      0.00000000  0.00000000  1.00000000 -1.35225296
      0.00000000  0.00000000  0.00000000  1.00000000
\end{Verbatim}

\subsubsection{{\tt matrixf\_ludecomp\_doolittle()} (LU Decomposition, Doolittle's Method)}
\label{module:matrix:ludecomp_doolittle}
Doolittle's method is similar to Crout's except it is the lower triangular
matrix that is left with ones on the diagonal.
The update algorithm is similar to Crout's but with a slight variation: the
upper triangular matrix is computed first.
The {\tt matrixf\_ludecomp\_doolittle(*A,m,n,*L,*U,*P)} implements
Doolittle's method.
%
\[
    \vec{U}_{k,j} = \vec{A}_{k,j} -
                    \sum_{t=0}^{k-1}{ \vec{L}_{k,t} \vec{U}_{t,j} }
                    \ \forall k \in \{0,n-1\}, j \in \{k,n-1\}
\]
%
\[
    \vec{L}_{i,k} = \left[
                        \vec{A}_{i,k} -
                        \sum_{t=0}^{k-1}{ \vec{L}_{i,t} \vec{U}_{t,k} }
                    \right] / \vec{U}_{k,k}
                    \ \forall k \in \{0,n-1\}, i \in \{k+1,n-1\}
\]
%
Here is a simple example:
\begin{Verbatim}[fontsize=\small]
    matrixf_ludecomp_doolittle(X,4,4,L,U,P)
    L =
      1.00000000  0.00000000  0.00000000  0.00000000
      4.73412609  1.00000000  0.00000000  0.00000000
      8.62828636  1.52072513  1.00000000  0.00000000
      7.20225906  1.08797777  0.80051047  1.00000000
    U =
      0.84381998 -2.38303995  1.43060994 -1.66603994
      0.00000000 12.16227150 -2.07895803  8.33287334
      0.00000000  0.00000000 -8.51144791 11.50963116
      0.00000000  0.00000000  0.00000000 -6.70172977
\end{Verbatim}

\subsubsection{{\tt matrixf\_qrdecomp\_gramschmidt()}
               (QR Decomposition, Gram-Schmidt algorithm)}
\label{module:matrix:qrdecomp_gramschmidt}
\liquid\ implements Q/R decomposition with the
{\tt matrixf\_qrdecomp\_gramschmidt(*A,m,n,*Q,*R)} method which factors
a non-singular $n \times n$ matrix $\vec{A}$ into
product of an orthogonal matrix $\vec{Q}$ and an upper triangular matrix
$\vec{R}$, each $n \times n$.
That is, $\vec{A} = \vec{Q}\vec{R}$ where
$\vec{Q}^T\vec{Q} = \vec{I}_n$ and
$\vec{R}_{i,j} = 0\,\, \forall_{i > j}$.
%
Building on the previous example for our test $4 \times 4$ matrix
$\vec{X}$, the Q/R factorization is
%
\begin{Verbatim}[fontsize=\small]
    matrixf_qrdecomp_gramschmidt(X,4,4,Q,R)
    Q =
      0.08172275 -0.57793844  0.57207584  0.57622749
      0.38688579  0.63226062  0.66619849 -0.08213031
      0.70512730  0.13563085 -0.47556636  0.50816941
      0.58858842 -0.49783322  0.05239720 -0.63480729
    R =
     10.32539940 -3.62461853  3.12874746  6.70309162
      0.00000000  3.61081028  1.62036073  2.78449297
      0.00000000  0.00000000  3.69074893 -5.34197950
      0.00000000  0.00000000  0.00000000  4.25430155
\end{Verbatim}

\subsubsection{{\tt matrixf\_chol()}
               (Cholesky Decomposition)}
\label{module:matrix:chol}
Compute Cholesky decomposition of an $n \times n$ symmetric/Hermitian
positive-definite matrix as $\vec{A} = \vec{L}\vec{L}^T$
where $\vec{L}$ is $n \times n$ and lower triangular.
An $n \times n$ matrix is positive definite if
$\Re\{v^T \vec{A} v\} > 0$ for all non-zero vectors $v$.
Note that $\vec{A}$ can be either complex or real.
%
Shown below is an example of the Cholesky decomposition of a
$4 \times 4$ positive definite real matrix.
%
\begin{Verbatim}[fontsize=\small]
    A =
      1.0201000  -1.4341999   0.3232000  -1.0302000
     -1.4341999   2.2663999   0.5506001   1.2883999
      0.3232000   0.5506001   4.2325001  -1.4646000
     -1.0302000   1.2883999  -1.4646000   5.0101995
    matrixf_chol(A,4,Lp)
      1.0100000   0.0000000   0.0000000   0.0000000
     -1.4200000   0.5000000   0.0000000   0.0000000
      0.3200000   2.0100000   0.3000003   0.0000000
     -1.0200000  -0.3199999  -1.6499993   1.0700010
\end{Verbatim}



\subsubsection{{\tt matrixf\_gjelim()} (Gauss-Jordan Elimination)}
\label{module:matrix:gjelim}
The {\tt matrixf\_gjelim(*X,m,n)} method in \liquid\
performs the Gauss-Jordan elimination on a matrix $\vec{X}$.
Gauss-Jordan elimination converts a $m \times n$ matrix into its
row-reduced echelon form using elementary matrix operations
(e.g. pivoting).
This can be used to solve a linear system of $n$ equations
$\vec{A}\vec{x} = \vec{b}$ for the unknown vector $\vec{x}$
\[
    \begin{bmatrix}
        A_{0,0}     & A_{0,1}   & \cdots  & A_{0,n-1} \\
        A_{1,0}     & A_{1,1}   & \cdots  & A_{1,n-1} \\
        \\
        A_{n-1,0}   & A_{n-1,1} & \cdots  & A_{n-1,n-1}
    \end{bmatrix}
    \begin{bmatrix}
        x_{0} \\
        x_{1} \\
        \\
        x_{n-1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        b_{0} \\
        b_{1} \\
        \\
        b_{n-1}
    \end{bmatrix}
\]
The solution for $\vec{x}$ is given by inverting $\vec{A}$ and multiplying
by $\vec{b}$, viz
\[
    \vec{x} = \vec{A}^{-1}\vec{b}
\]
This is also equivalent to augmenting $\vec{A}$ with $\vec{b}$ and
converting it to its row-reduced echelon form.
If $\vec{A}$ is non-singular the resulting $n \times n+1$ matrix will hold
$\vec{x}$ in its last column.
The row-reduced echelon form of a matrix is computed in {\it liquid} using the
Gauss-Jordan elimination algorithm, and can be invoked as such:
\begin{Verbatim}[fontsize=\small]
    Ab =
      0.84381998 -2.38303995  1.43060994 -1.66603994  0.91488999
      3.99475002  0.88066000  4.69372988  0.44563001  0.71789002
      7.28072023 -2.06608009  0.67074001  9.80657005  1.06552994
      6.07741022 -3.93098998  1.22826004 -0.42142001 -0.81707001
    matrixf_gjelim(Ab,4,5)
      1.00000000 -0.00000000  0.00000000 -0.00000000 -0.51971692
     -0.00000000  1.00000000  0.00000000  0.00000000 -0.43340963
     -0.00000000 -0.00000000  1.00000000 -0.00000000  0.64247853
      0.00000000 -0.00000000 -0.00000000  0.99999994  0.35925382
\end{Verbatim}
Notice that the result contains $\vec{I}_n$ in its first $n$ rows and $n$
columns (to within machine precision).%
\footnote{row permutations (swapping) might have occurred.}

