% 
% MODULE : equalization
%

\newpage
\section{equalization}
\label{module:equalization}
This section describes the equalizer module and the functionality of two
digital linear adaptive equalizers implemented in \liquid, LMS and RLS.
Their interfaces are nearly identical;
however their internal functionality is quite different.
Specifically the LMS algorithm is less computationally complex but is
slower to converge than the RLS algorithm.

\subsection{System Description}
\label{module:equalization:system}
Suppose a known transmitted symbol sequence
$\vec{d} = [ d(0), d(1), \ldots ,d(N-1) ]$
which passes through an unknown channel filter $\vec{h}_n$ of length
$q$.
The received symbol at time $n$ is therefore
%
\begin{equation}
    y(n) = \sum\limits_{k=0}^{q-1}{h_n(k)d(n-k)} + \varphi(n)
\end{equation}
%
where $\varphi(n)$ represents white Gauss noise.
The adaptive linear equalizer attempts to use a finite impulse response (FIR)
filter $\vec{w}$ of length $p$ to estimate the transmitted symbol, using only
the received signal vector $\vec{y}$ and the known data sequence $\vec{d}$,
viz
%
\begin{equation}
\label{eqn:equalization:dhat}
    \hat{d}(n) = \vec{w}_n^T \vec{y}_n
\end{equation}
%
where $\vec{y}_n = [ y(n), y(n-1),\ldots, y(n-p+1) ]^T$.
Several methods for estimating $\vec{w}$ are known in the literature, and
typically rely on iteratively adjusting $\vec{w}$ with each input though a
recursion algorithm.
This section provides a very brief overview of two prevalent adaptation
algorithms;
for a more in-depth discussion the interested reader is referred to
\cite{Proakis:2001,Haykin:2002}.

\subsection{{\tt eqlms} (least mean-squares equalizer)}
\label{module:equalization:eqlms}
The least mean-squares (LMS) algorithm adapts the coefficients of the filter
estimate using a steepest descent (gradient) of the instantaneous {\it a priori}
error.
The filter estimate at time $n+1$ follows the following recursion
%
\begin{equation}
\label{eqn:equalization:lms_update}
    \vec{w}_{n+1} = \vec{w}_{n} - \mu \vec{g}_n
\end{equation}
%
where $\mu$ is the iterative step size, and
$\vec{g}_n$ the normalized gradient vector, estimated from the error signal
and the coefficients vector at time $n$.

\subsection{{\tt eqrls} (recursive least-squares equalizer)}
\label{module:equalization:eqrls}
The recursive least-squares (RLS) algorithm attempts to minimize the
time-average weighted square error of the filter output, viz
\begin{equation}
    c(\vec{w}_n) = \sum\limits_{i=0}^{n}{ \lambda^{i-n} \left| d(i)-\hat{d}(i)\right|^2 }
\end{equation}
where the forgetting factor $0<\lambda\leq 1$ which introduces
exponential weighting into past data, appropriate for time-varying
channels.
The solution to minimizing the cost function $c(\vec{w}_n)$ is achieved by
setting its partial derivatives with respect to $\vec{w}_n$ equal to zero.
The solution at time $n$ involves inverting the weighted cross correlation
matrix for $\vec{y}_n$, a computationally complex task.
This step can be circumvented through the use of a recursive algorithm which
attempts to estimate the inverse using the {\it a priori} error from the
output of the filter.
The update equation is
%
\begin{equation}
\label{eqn:equalization:rls_update}
    \vec{w}_{n+1} = \vec{w}_n + \Delta_{n}
\end{equation}
%
where the correction factor $\Delta_{n}$ depends on $\vec{y}_n$ and $\vec{w}_n$,
and involves several $p \times p$ matrix multiplications.
The RLS algorithm provides a solution which converges much faster than the LMS
algorithm, however with a significant increase in computational complexity and
memory requirements.

\subsection{Interface}
\label{module:equalization:interface}
The {\tt eqlms} and {\tt eqrls} have nearly identical interfaces so we will
leave the discussion to the {\tt eqlms} object here.
Like most objects in \liquid, {\tt eqlms} follows the typical
{\tt create()}, {\tt execute()}, {\tt destroy()} life cycle.
Training is accomplished either one sample at a time, or in a batch cycle.
If trained one sample at a time, the symbols must be trained in the proper
order, otherwise the algorithm won't converge.
One can think of the equalizer object in \liquid\ as simply a
{\tt firfilt} object (finite impulse response filter)
which has the additional ability to modify its own internal coefficients
based on some error criteria.
%
Listed below is the full interface to the {\tt eqlms} family of
objects.
While each method is listed for {\tt eqlms\_cccf}, the same
functionality applies to {\tt eqlms\_rrrf}
as well as the RLS equalizer objects
({\tt eqrls\_cccf} and {\tt eqrls\_rrrf}).
%
\begin{description}
\item[{\tt eqlms\_cccf\_create(*h,n)}]
    creates and returns an equalizer object with $n$ taps,
    initialized with the input array $\vec{h}$.
    If the array value is set to the {\tt NULL} pointer then the
    internal coefficients are initialized to
    $\{1,0,0,\ldots,0\}$.
\item[{\tt eqlms\_cccf\_destroy(q)}]
    destroys the equalizer object, freeing all internally-allocated
    memory.
\item[{\tt eqlms\_cccf\_print(q)}]
    prints the internal state of the {\tt eqlms} object.
\item[{\tt eqlms\_cccf\_set\_bw(q,w)}]
    sets the bandwidth of the equalizer to $w$.
    For the LMS equalizer this is the learning parameter $\mu$ which has
    a default value of 0.5.
    For the RLS equalizer the ``bandwidth'' is the forgetting factor
    $\lambda$ which defaults to 0.99.
\item[{\tt eqlms\_cccf\_reset(q)}]
    clears the internal equalizer buffers and sets the internal
    coefficients to the default
    (those specified when {\tt create()} was invoked).
\item[{\tt eqlms\_cccf\_push(q,x)}]
    pushes a sample $x$ into the internal buffer of the equalizer
    object.
\item[{\tt eqlms\_cccf\_execute(q,*y)}]
    generates the output sample $y$ by computing the vector dot product
    (see \S\ref{module:dotprod})
    between the internal filter coefficients and the internal buffer.
\item[{\tt eqlms\_cccf\_step(q,d,d\_hat)}]
    performs a single iteration of equalization with an estimated output
    $\hat{d}$ for an expected output $d$.
    The weights are updated internally defined by
    (\ref{eqn:equalization:lms_update}) for the LMS equalizer and
    (\ref{eqn:equalization:rls_update}) for the RLS equalizer.
\item[{\tt eqlms\_cccf\_get\_weights(q,*w)}]
    returns the internal filter coefficients (weights) at the current
    state of the equalizer.
\end{description}
%
Here is a simple example:
%
\input{listings/eqlms_cccf.example.c.tex}
%
For more detailed examples, see
{\tt examples/eqlms\_cccf\_example.c} and
{\tt examples/eqrls\_cccf\_example.c}.



\subsection{Blind Equalization}
\label{module:equalization:blind}
%
The equalizer interface above permits decision-directed equalization.
This is a form of blind equalization where the data are not known,
but the modulation scheme is.
This type of equalization is useful for adapting to channel conditions,
matched-filter ISI imperfections,
and small timing offsets.
Listed below is a basic program to equalize to a BPSK signal with
unknown data.
%
\input{listings/eqlms_cccf_blind.example.c.tex}
%
The equalizer filter is initialized with square-root raised-cosine
coefficients
(see \S\ref{module:filter:firdes:rnyquist} for details of square-root
Nyquist filter designs).
After computing each output symbol,
the transmitted symbol is estimated and
the equalizer adjusts its coefficients internally using the
{\tt step()} method.
This can be easily combined with the linear {\tt modem} object's
{\tt modem\_get\_demodulator\_sample()}
interface to return the estimated symbol after demodulation
(see \S\ref{module:modem:digital}).

%-------------------- FIGURE: EQLMS BLIND --------------------
\begin{figure}
\centering
\subfigure[Equalizer output (time series)] {
    \includegraphics[trim = 2mm 0mm 0mm 0mm, clip, width=15cm]{figures.gen/eqlms_cccf_blind_time}
    \label{fig:module:equalization:eqlms_blind:time}
}
%\subfigure[mean-squared error] {
%    \includegraphics[trim = 0mm 23mm 0mm 23mm, clip, width=13cm]{figures.gen/eqlms_cccf_blind_mse}
%    \label{fig:module:equalization:eqlms_blind:mse}
%}
\subfigure[Power Spectral Density] {
    \includegraphics[trim = 2mm 6mm 0mm 6mm, clip, width=13cm]{figures.gen/eqlms_cccf_blind_freq}
    \label{fig:module:equalization:eqlms_blind:freq}
}
%\subfigure[constellation] {
%    \includegraphics[trim = 16mm 0mm 18mm 0mm, clip, width=7cm]{figures.gen/eqlms_cccf_blind_const}
%    \label{fig:module:equalization:eqlms_blind:constellation}
%}

% trim = left bottom right top
\caption{Blind {\tt eqlms\_cccf} example, $k=2$ samples/symbol}
\label{fig:module:equalization:eqlms_blind}
\end{figure}
%
An example of the decision-directed equalizer for a QPSK signal with
unknown data is depicted in
Figure~\ref{fig:module:equalization:eqlms_blind}.
A QPSK signal filtered with a square-root raised-cosine filter is
transmitted through a noisy channel with several multi-path components,
contributing to inter-symbol interference.
The receiver uses an equalizer initialized with a matched filter.
The output time series in
Figure~\ref{fig:module:equalization:eqlms_blind:time}
shows that the first 200 symbols are particularly noisy with a
significant amount of inter-symbol interference due to the effects of
the channel.
The equalizer, however, quickly adapts and removes most of the
interference as can be seen in
Figure~\ref{fig:module:equalization:eqlms_blind:freq}
(the {\em composite} spectrum is nearly flat in the pass-band).
%
For a more detailed example, see
{\tt examples/eqlms\_cccf\_blind\_example.c}
located under the main project directory.

\subsection{Comparison of {\tt eqlms} and {\tt eqrls} Object Families}
\label{module:equalization:eqlms_vs_eqrls}
The performance of the {\tt eqlms} and {\tt eqrls} equalizers are compared by
generating a channel with an impulse response representing a strong
line-of-sight (LoS) component followed by random echoes.
Each was trained on 512 iterations of a known QPSK-modulated training sequence
with learning rate parameters $\mu=0.999$ and $\lambda=0.999$ for the LMS and
RLS algorithms, respectively.
A small amount of noise was injected after the channel filter to demonstrate
the robustness of the algorithms.
The results of two simulations are shown in
Figure~\ref{fig:module:equalization:eqlms_vs_eqrls}
demonstrating a 10-tap equalizer applied to the response of a 6-tap
channel with an SNR of 40~dB.

The pass-band power spectral densities (PSD) of the channel and the equalizer
outputs are depicted in
Figure~\ref{fig:module:equalization:eqlms_vs_eqrls:psd}.
Notice that the inter-symbol interference of the channel causes its PSD to
have a non-flat response.
Theoretically, if the inter-symbol interference is completely removed, the
response of both the channel and the equalizer will be completely flat
(neglecting any noise present).
While the PSD of the equalized output is nearly flat in the figure,
it is important to realize that these algorithms minimize a cost function
defined as the square of the {\it a priori} filter output error, and do not
necessarily force the PSD to zero.
The classic zero-forcing equalizer has several drawbacks:
\begin{enumerate}
\item the equalizing filter which would give this response is not
      necessarily realizable; that is, not all channels can be
      perfectly inverted,
\item forcing the frequency response to zero increases the noise
      terms of frequencies where the spectra of the channel
      response is low.  In this regard, the zero-forcing equalizer
      only reduces inter-symbol interference and does not maximize
      the ratio of signal power to both interference {\it and}
      noise power as the LMS and RLS algorithms do.
\end{enumerate}
It is interesting to note that both the LMS and RLS equalizers converge to
nearly the same solution.
The RLS equalizer, however, has a slightly lower error after
training while converging to its error minimum much faster.
The RLS equalizer, however, has a much higher computational complexity.

%-------------------- FIGURE: eqlms vs. eqrls --------------------
\begin{figure}
\centering
\mbox{
  \subfigure[PSD] {
      \includegraphics[trim = 16mm 0mm 16mm 0mm, clip, width=7.5cm]{figures.gen/eqlms_vs_eqrls_freq}
      \label{fig:module:equalization:eqlms_vs_eqrls:psd}
    } \quad
  \subfigure[constellation] {
      \includegraphics[trim = 16mm 0mm 16mm 0mm, clip, width=7.5cm]{figures.gen/eqlms_vs_eqrls_const}
      \label{fig:module:equalization:eqlms_vs_eqrls:constellation}
    } \quad
}
\mbox{
  \subfigure[taps] {
      \includegraphics[trim = 23mm 0mm 23mm 0mm, clip, width=7.5cm]{figures.gen/eqlms_vs_eqrls_taps}
      \label{fig:module:equalization:eqlms_vs_eqrls:taps}
    } \quad
  \subfigure[mean-squared error] {
      \includegraphics[trim = 16mm 0mm 16mm 0mm, clip, width=7.5cm]{figures.gen/eqlms_vs_eqrls_mse}
      \label{fig:module:equalization:eqlms_vs_eqrls:mse}
    } \quad
}
% trim = left bottom right top
\caption{Comparison of 10-tap {\tt eqlms\_cccf} and {\tt eqrls\_cccf}
         equalizer objects for a 6-tap channel with 40~dB SNR}
\label{fig:module:equalization:eqlms_vs_eqrls}
\end{figure}



